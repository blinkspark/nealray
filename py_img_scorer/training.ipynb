{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import layers, activations\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from concurrent import futures\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "trainingDataDir = 'o:/temp/pixiv/training/'\n",
    "targetSize = (224, 224)\n",
    "targetShape = (224, 224, 3)\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(\n",
    "    dataPath,\n",
    "    batchSize=32,\n",
    "    shuffle=True,\n",
    "    seed=1,\n",
    "    train_split=0.7,\n",
    "    #  test_split=0.2,\n",
    "    valid_split=0.3,\n",
    "    subset='train',\n",
    "    print_len=False,\n",
    "    targetSize=(224, 224)):\n",
    "    test_split = 1 - train_split - valid_split\n",
    "    csvPath = path.join(trainingDataDir, 'data.csv')\n",
    "\n",
    "    if not path.exists(dataPath):\n",
    "        raise Exception('dataPath not found')\n",
    "    dataCsv = pd.read_csv(csvPath, index_col='img')\n",
    "    dataCsv = dataCsv.dropna(subset='R18')\n",
    "    # positiveCount = dataCsv['R18'].eq(1).sum()\n",
    "    # negativeCount = dataCsv['R18'].eq(0).sum()\n",
    "    # count = min(positiveCount, negativeCount)\n",
    "    # negativeDatas = dataCsv[dataCsv['R18'].eq(0)].sample(count,\n",
    "    #                                                      random_state=seed)\n",
    "    # positiveDatas = dataCsv[dataCsv['R18'].eq(1)].sample(count,\n",
    "    #                                                      random_state=seed)\n",
    "    # dataCsv = pd.concat([negativeDatas, positiveDatas])\n",
    "\n",
    "    if shuffle:\n",
    "        dataCsv = dataCsv.sample(frac=1, random_state=seed)\n",
    "    if subset == 'train':\n",
    "        dataCsv = dataCsv[:int(train_split * len(dataCsv))]\n",
    "    elif subset == 'valid':\n",
    "        dataCsv = dataCsv[int(train_split *\n",
    "                              len(dataCsv)):int((train_split + valid_split) *\n",
    "                                                len(dataCsv))]\n",
    "    elif subset == 'test':\n",
    "        dataCsv = dataCsv[int((train_split + valid_split) * len(dataCsv)):]\n",
    "    else:\n",
    "        raise Exception('subset must be train or test')\n",
    "    if len(dataCsv) == 0:\n",
    "        raise Exception(\"dataset is zero\")\n",
    "    if print_len:\n",
    "        print(len(dataCsv))\n",
    "    count = 0\n",
    "    imgPaths = []\n",
    "    ys = []\n",
    "    with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "\n",
    "        def load_img(imgPath):\n",
    "            img = Image.open(imgPath)\n",
    "            xx = max(img.size)\n",
    "            newImg = Image.new('RGB', (xx, xx))\n",
    "            newImg.paste(img, (int(\n",
    "                (xx - img.size[0]) / 2), int((xx - img.size[1]) / 2)))\n",
    "            newImg = newImg.resize(targetSize, Image.BICUBIC)\n",
    "            return np.array(newImg)\n",
    "\n",
    "        while True:\n",
    "            for i in dataCsv.index:\n",
    "                imgPath = path.join(trainingDataDir, i)\n",
    "                imgPaths.append(imgPath)\n",
    "                ys.append(dataCsv.loc[i, 'R18'])\n",
    "                count += 1\n",
    "                if count == batchSize:\n",
    "                    res = executor.map(load_img, imgPaths)\n",
    "                    yield np.array(list(res)), np.array(ys)\n",
    "                    count = 0\n",
    "                    imgPaths = []\n",
    "                    ys = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobNet = keras.applications.mobilenet_v2.MobileNetV2(include_top=False,\n",
    "                                                     weights='imagenet',\n",
    "                                                     input_shape=targetShape)\n",
    "for layer in mobNet.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLayer = layers.Input(shape=targetShape)\n",
    "# preprocess\n",
    "x = layers.RandomFlip()(inputLayer)\n",
    "x = layers.RandomZoom(height_factor=0.2, width_factor=0.2)(x)\n",
    "x = layers.RandomRotation(0.2)(x)\n",
    "x = layers.Rescaling(scale=1. / 127.5, offset=-1)(x)\n",
    "x = mobNet(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(2048)(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(2048)(x)\n",
    "x = layers.ReLU()(x)\n",
    "outputLayer = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs=inputLayer, outputs=outputLayer)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                  patience=10,\n",
    "                                  verbose=1,\n",
    "                                  restore_best_weights=True)\n",
    "]\n",
    "model.fit(data_gen(trainingDataDir,\n",
    "                   train_split=0.8,\n",
    "                   valid_split=0.1,\n",
    "                   batchSize=64),\n",
    "          batch_size=64,\n",
    "          steps_per_epoch=30,\n",
    "          epochs=1000,\n",
    "          class_weight={\n",
    "              0: 2.315,\n",
    "              1: 1\n",
    "          },\n",
    "          validation_data=data_gen(trainingDataDir,\n",
    "                                   batchSize=64,\n",
    "                                   subset='valid',\n",
    "                                   train_split=0.8,\n",
    "                                   valid_split=0.1),\n",
    "          validation_steps=4,\n",
    "          max_queue_size=30,\n",
    "          callbacks=cbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = data_gen(trainingDataDir,\n",
    "               subset='test',\n",
    "               batchSize=64,\n",
    "               train_split=0.8,\n",
    "               valid_split=0.1,\n",
    "               print_len=True)\n",
    "# next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(gen,batch_size=64,steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, ty = next(gen)\n",
    "res = model.predict(tx)\n",
    "for i in range(len(res)):\n",
    "    plt.imshow(tx[i])\n",
    "    plt.show()\n",
    "    print(res[i], ty[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model-r18.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2a765c70a93b94e6c6308bc02fe48034fea3f1086e8d50b596d4a63fb95b1ef2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
